"""
This scripts aims to generate statistics and plots for the accession data generated by HMMER.
In particular, it will do this by varing two main parameters: 1) e-value and 2) Jaccard similarity threshold.
These two parameters will be varied in order to determine the optimal values for each (if such a value exists).
And morevoer, have a stronger understanding of the distribution of the data as it goes through the pipeline 
especially machine learning.

Environment Variables to set:
    LOGLEVEL: logging level for the script (default: INFO, options: DEBUG, INFO, WARNING, ERROR, CRITICAL)

Note:
    This is a script meant to run at base level of the repository. e.g. 'python testing/ana-accession_stats.py'
    It also assumes that 'train_model.py' (and associated scripts) have been run and the data is in the correct location.
    In general, I assume the 50k dataset is used for this analysis. So if you want to use a different dataset, you will need to change the paths
    and db names accordingly.

Future Work:
    - [ ] make paths options with default values
    - [ ] LOGLEVEL should be in the CLI (and not env var)
    - [ ] think about a better data structure for evalue and jaccard threshold values  
    - [ ] use os more for paths stuff specifically for assembling paths
        os.path.join(path1, path2, path3, ...)
    - [x] shift everything right as jaccard threshold is not a parameter for hmmsearch
    - [ ] use count and collections to estimate the number of proteins in pairs true and falses (e.g. how many proteins are in pairs) divide by the total number of proteins
        to get the frequency of proteins in pairs that way we can see how e_value and jaccard threshold affect the number of proteins in pairs
Suggestion:
    Do the cursor. Save the chunk of the df locally. (caching)
"""
# system dependencies
import os
import logging
from collections import Counter


# library dependencies
import click
import duckdb as ddb

import numpy as np
import pandas as pd
import pyhmmer
from sklearn.metrics import log_loss, roc_curve
from tqdm import tqdm



# local dependencies
from pairpro.preprocessing import connect_db, build_pairpro
from pairpro.user_blast import make_blast_df

import pairpro.hmmer as pp_hmmer
import pairpro.utils as pp_utils

####################
### PATHS & VARS ###
####################

# Paths
HMM_PATH = './data/pfam/Pfam-A.hmm'
DB_PATH = './tmp/pairpro.db'
TRUE_LABEL_PATH = 'OMA-pairpro_cross.db'
PRESS_PATH = './data/pfam/pfam'

ANALYSIS_OUTPUT_PATH = './data/analysis/'
HMMER_OUTPUT_DIR = './data/analysis/hmmer/'
PARSE_HMMER_OUTPUT_DIR = './data/analysis/hmmer_parsed/'


# venv variables
if 'LOGLEVEL' in os.environ:
    LOGLEVEL = os.environ['LOGLEVEL']
else:
    LOGLEVEL = 'DEBUG' # change to INFO for production
LOGNAME = __file__
LOGFILE = f'./logs/{os.path.basename(__file__)}.log'

####################



####################
###    SCRIPT    ###
####################


@click.command()
@click.option('--chunk_size', default=1,
              help='Number of sequences to process in each chunk')
@click.option('--njobs', default=4,
              help='Number of parallel processes to use for HMMER')
@click.option('--evalue', default=1.e-10,
                help='E-value for HMMER')
@click.option('--jaccard_threshold', default=0.7,
              help='Jaccard threshold for filtering protein pairs')
@click.option('--vector_size', default=1,
              help='Size of the vector for the dataframe chunking') 
def analysis_script(chunk_size, njobs, evalue, jaccard_threshold, vector_size, **kwargs):
    """
    This a wrapper function that will run the analysis for the accession data.
    """
    # connect to database
    con = ddb.connect(DB_PATH, read_only=False)

    # get number of proteins in pairs
    proteins_in_pair_count = con.execute(f"SELECT COUNT(*) FROM pairpro.pairpro.proteins").fetchone()[0]
    logger.debug(
        f"Total number of protein in pairs: {proteins_in_pair_count} in pipeline")

    # get proteins in pairs
    proteins_in_pair = con.execute(
        f"SELECT pid, protein_seq FROM pairpro.pairpro.proteins")
    
    # create empty lists to store statistics
    evalue_values = []
    all_results = [] # initialize the list to store all the results 

    
    # Loop over e-value values
    for evalue_value in evalue_values_to_test:

        logger.info(f"Running analysis for e-value: {evalue_value}")
        ### Run HMMER ###
        # get number of hmms for evalue calc
        profiles = list(pyhmmer.plan7.HMMFile(HMM_PATH))
        n_hmms = len(profiles)
        del profiles
        logger.info(f"Number of HMMs: {n_hmms}")

        # run hmmsearch
        targets = pp_hmmer.prefetch_targets(PRESS_PATH)
        logger.info(f"Number of targets: {len(targets)}")
        wrapper = lambda chunk_index, pid_chunk: pp_hmmer.local_hmmer_wrapper(
            chunk_index, pid_chunk, press_path=PRESS_PATH, hmm_path=HMM_PATH, out_dir=HMMER_OUTPUT_DIR, cpu=njobs, prefetch=targets, e_value=evalue_value, scan=False, Z=n_hmms)
        
        # get proteins in pairs
        proteins_in_pair = con.execute(
            f"SELECT pid, protein_seq FROM pairpro.pairpro.proteins")
        
        # the hmmsearch loop
        complete = False
        chunk_index = 0
        total_processed = 0
        # use tqdm to track progress
        pbar = tqdm(total=proteins_in_pair_count)
        while not complete:
            pid_chunk = proteins_in_pair.fetch_df_chunk(vectors_per_chunk=chunk_size)
            logger.info(f"Loaded chunk of size {len(pid_chunk)}")
            if len(pid_chunk) == 0:
                complete = True
                break
            wrapper(chunk_index, pid_chunk)
            logger.info(f"Ran chunk, validating results")

            df = pd.read_csv(f'{HMMER_OUTPUT_DIR}/{chunk_index}_output.csv')
            assert set(list(pid_chunk['pid'].values)) == set(list(df['query_id'].values)), "Not all query ids are in the output file"

            logger.info(f"Completed chunk {chunk_index} with size {len(pid_chunk)}")
            total_processed += len(pid_chunk)
            chunk_index += 1

            # update progress bar
            pbar.update(len(pid_chunk))
        pbar.close()


        logger.info('Finished running HMMER.')

        # parse the output
        logger.info('Parsing HMMER output...')
        
        # arbitrary jaccard threshold
        jaccard_threshold_value = 0.5

        # setup the database and get some pairs to run
        con.execute("""
            CREATE OR REPLACE TABLE proteins_from_pairs4 AS
            SELECT query_id AS pid, accession_id AS accession
            FROM read_csv_auto('./data/analysis/hmmer/*.csv', HEADER=TRUE)
        """)
        con.commit()

        logger.info('creating pair table')
        pp_hmmer.process_pairs_table_ana(
            con,
            'pairpro',
            vector_size,
            PARSE_HMMER_OUTPUT_DIR,
            jaccard_threshold_value)

        logger.info('Finished parsing HMMER output.')

        logger.info('Finding ground truth...')
        ## load HMMER output data
        parse_list = [] # initialize list to store dataframes
        for chunk_index in range(1, chunk_index):
            output_file_path = f'{PARSE_HMMER_OUTPUT_DIR}{chunk_index}_output.csv'     
            df = pd.read_csv(output_file_path)
            logger.debug(f"Loaded chunk {chunk_index} with size {len(df)}")
            parse_list.append(df)
        
        logger.debug(f"parse_list: {parse_list}")
        hmmer_df = pd.concat(parse_list, ignore_index=True)

        ## load the true labels
        conn = ddb.connect(TRUE_LABEL_PATH, read_only=False)
        true_pairs_query = 'SELECT meso_pid, thermo_pid FROM pairs'
        true_pairs_df = conn.execute(true_pairs_query).fetch_df()

        ## create 'true pairs' column based on merge
        merged_df = pd.merge(hmmer_df, true_pairs_df,
                            how='left', left_on=['meso_pid', 'thermo_pid'], right_on=['meso_pid', 'thermo_pid'])

        # create "true pairs" column based                   
        merged_df['true_pairs'] = merged_df['meso_pid'].notnull() & merged_df['thermo_pid'].notnull()

        # save the merged dataframe
        merged_df.to_csv(f'{ANALYSIS_OUTPUT_PATH}merged_df.csv', index=False)

        # continue from here
        

    

    


if __name__ == "__main__":
    # Initialize logger
    logger = pp_utils.start_logger_if_necessary(
        LOGNAME, LOGFILE, LOGLEVEL, filemode='w')
    hmmer_logger = logging.getLogger('pairpro.hmmer')
    hmmer_logger.setLevel(getattr(logging, LOGLEVEL))
    hmmer_logger.addHandler(logging.FileHandler(LOGFILE))
    logger.info(f"Running {__file__}")

    # create analysis directory
    try:
        os.makedirs(ANALYSIS_OUTPUT_PATH, exist_ok=True)
    except OSError as e:
        logger.error(f'Error creating directory: {e}')
    
    # create HMMER output directory
    try:
        os.makedirs(HMMER_OUTPUT_DIR, exist_ok=True)
    except OSError as e:
        logger.error(f'Error creating directory: {e}')

    # create parsed HMMER output directory
    try:
        os.makedirs(PARSE_HMMER_OUTPUT_DIR, exist_ok=True)
    except OSError as e:
        logger.error(f'Error creating directory: {e}')


    ## assume that the data is already in the correct location
    ## the hmm's have been also pressed

    # define the range of e-value to test
    evalue_values_to_test = [1e-10, 1e-5, 1e-3, 1e-1]


    # run the analysis
    analysis_script()

    
