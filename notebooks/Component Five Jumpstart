s5.0_relation.py

Params: Population Distribution

Inputs: Pandas Dataframe containing Pfam return data. Includes quantitative features (ID, some metric of percent similarlity) and string of amino acid sequence.

Outputs: Quantitative functional similarlity metric.

Metrics:

Packages: Pandas, numpy, scipy, seaborn, fuzzywuzzy

Potential Tests

**Right now, this is specific to input only from Pfam. Will need to expand once other softwares are introduced**

Inputs: 
1) Test for pandas dataframe
2) Test that columns we don't want (ID, index) are removed, data needs to be cleaned before this step
3) Test that there are none or minimal null values (have to discuss)
4) Maybe most important - test that two string sequences exist for pair of proteins entered
5) Does sample distro match population?


Outputs:
1) Test that data type is float or int (score)
2) Test that confidence test worked (maybe run same test on known set of data)


Some components (or sub-components): 

**first, components for training model with our own data**
-----------------------------------------------------------------------------------------
Component 1: Test for pandas dataframe input
Use case: User takes data from component 4 (where data is processed into pandas dataframe) and wants to pass it into relationship component.

def check_input_type(sample_data):
    tests that input data is a pandas dataframe with assert statement. 
    Output should pass unless assert statement fails.
-----------------------------------------------------------------------------------------    
Component 2: Checks that input data is cleaned property (does it have all of the features we need, and are the features we don't need removed).
Use case: Input data does not include local E value, which we need as an input to our model.

def check_input_cleaning(sample_data):
    searches for each column by string
    includes if statement that raises an error if a column that shouldn't be there (e.g. ID#) is
    Output: returns value count for columns we do need, and raises an error if we are missing data
    Clean out NAN's
-----------------------------------------------------------------------------------------
Component 3: Checks that distribution of input data follows sample distribution
Use case:

def compare_pop_to_sample(sample_data, param = pop_distr):
     takes in the sample dataframe along with a population distribution. Thinking that population distribution should 
     be calculated in an earlier part of the workflow. This is only one value, so we can pass it as a parameter.
     Function should run a Jensen-Shannon divergence test on data.
     Output: returns a confidence that distributions are similar, print('OK!') or ('not OK!)
-----------------------------------------------------------------------------------------
**This is functional code from this point on, not just tests.**
-----------------------------------------------------------------------------------------
Component 4: Encode nominal features from sample database
Use case: Sample database has important nominal features (amino acids sequences). 
  
def encode_nominal features(sample_data):
    adjust Evan's code for this. Uses OneHotEncoding (or other form) to quantify nominal features of the database. 
    Output: Returns an array of 1's and 0's for each feature.
    Note: We may be able to skip this step if we can conclude that quantitative features native to Pfam cover everything
    about the sequence that we are interested in.
-----------------------------------------------------------------------------------------    
Component 5: Train the model with sample data.
Use case:

def train_model(sample_data):
    import scipy, numpy
    Split data into dev and test (0.8/0.2 for now)
    Train model (need to specify which kind of ML model we will use)
    Output: Print('Training successful!')

Test: Need some ideas here
    
-----------------------------------------------------------------------------------------
Component 6: Test the model with sample data.
Use case:

def test_model(sample_data):
    Runs data through model
    Output: Returns model_score, confusion matrix

Test: Need ideas
-----------------------------------------------------------------------------------------
Component 7: Run confidence test on model output.
Use case:

def check_model_confidence(model_score, pop_score(something to compare to), ci_data):
    Runs a statistical test on model output and compares it to sample
    Output: Returns a confidence score along with the model score

Test: Run confidence test on some data for which we know the confidence score
      assert that the score is correct using numpy.isclose()
-----------------------------------------------------------------------------------------
Component 8: Calculate a 'functionality' metric that is the ultimate output of component five. Will need some more advanced 
             mathematical analysis here...
Use case: We need to test that our protein pairs have a near maximal functionality score! This can be used as a basis for 
          eventual useer input scores.

def calculate_functionality(model_score, sample_data):
    runs user input data through some mathematical manipulation of their model score and input data
    Output: returns a functionality score, print statement categorizing functionality score

Test: